{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building and Training a Feedforward Neural Network for Language Modeling**\n",
    "\n",
    "Estimated time needed: **60** minutes\n",
    "\n",
    "This project explores the use of Feedforward Neural Networks (FNNs) in language modeling. The primary objective is to build a neural network that learns word relationships and generates meaningful text sequences. The implementation is done using PyTorch, covering key aspects of Natural Language Processing (NLP), such as:\n",
    "* Tokenization & Indexing: Converting text into numerical representations.\n",
    "* Embedding Layers: Mapping words to dense vector representations for efficient learning.\n",
    "* Context-Target Pair Generation (N-grams): Structuring training data for sequence prediction.\n",
    "* Multi-Class Neural Network: Designing a model to predict the next word in a sequence.\n",
    "\n",
    "The training process includes optimizing the model with loss functions and backpropagation techniques to improve accuracy and coherence in text generation. By the end of the project, you will have a working FNN-based language model capable of generating text sequences.\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0WSVEN/song%20%281%29.png\" alt=\"Image Description\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-required-libraries\">Installing required libraries</a></li>\n",
    "            <li><a href=\"#Importing-required-libraries\">Importing required libraries</a></li>\n",
    "            <li><a href=\"#Defining-helper-functions\">Defining helper functions</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "       <a href=\"#Feedforward-Neural-Networks-(FNNs)-for-language-models\">Feedforward Neural Networks (FNNs) for language models</a></li>\n",
    "        <ol>\n",
    "            <li><a href=\"#Tokenization-for-FNN\">Tokenization for FNN</a></li>\n",
    "            <li><a href=\"#Indexing\">Indexing</a></li>\n",
    "            <li><a href=\"#Embedding-layers\">Embedding layers</a></li>\n",
    "        </ol>\n",
    "</li>\n",
    "    <li><a href=\"#Generating-context-target-pairs-(n-grams)\">Generating context-target pairs (n-grams)</a></li>\n",
    "    <ol>\n",
    "        <li><a href=\"#Batch-function\">Batch function</a></li>\n",
    "        <li><a href=\"#Multi-class-neural-network\">Multi-class neural network</a></li>\n",
    "    </ol>\n",
    "    <li><a href=\"#Training\">Training</a></li>\n",
    "    </li>\n",
    "    <li><a href=\"#Exercises\">Exercises</a></li>\n",
    "    </li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "\n",
    " - Implement a feedforward neural network using the PyTorch framework, including embedding layers, for language modeling tasks.\n",
    " - Fine-tune the output layer of the neural network for optimal performance in text generation.\n",
    " - Apply various training strategies and fundamental Natural Language Processing (NLP) techniques, such as tokenization and sequence analysis, to improve text generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All libraries required for this lab are listed below. The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the code__ in the cell below to install them. However, if you run this notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda), you may need to install additional libraries before proceeding with the lab.\n",
    "\n",
    "\n",
    "For this lab, you will use the following libraries:\n",
    "\n",
    "*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data.\n",
    "*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n",
    "*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine-learning-pipeline related functions.\n",
    "*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for visualizing the data.\n",
    "*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing required libraries\n",
    "\n",
    "<h3 style=\"color:red;\"> Note: Installing these packages may take several minutes. Please be patient and allow the installation to finish before proceeding.</h3>\n",
    "\n",
    "<h3 style=\"color:red;\">After installing the libraries below please RESTART THE KERNEL and run all cells.</h3>\n",
    "\n",
    "Note: If your environment doesn't support \"!pip install\", use \"!mamba install\"\n",
    "\n",
    "You may comment %%capture line below if you do not want to supress the messages that appear during package installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q numpy pandas matplotlib seaborn scikit-learn\n",
    "!pip install nltk\n",
    "!pip install torch==2.6.0\n",
    "!pip install torchtext==0.17.2\n",
    "!pip install torchvision==0.17.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries\n",
    "\n",
    "_It is recommended that you import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import string\n",
    "import time\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Networks (FNNs) for language models\n",
    "\n",
    "FNNs, or Multi-Layer Perceptrons, serve as the foundational components for comprehending neural networks in natural language processing (NLP). In NLP tasks, FNNs process textual data by transforming it into numerical vectors known as embeddings. Subsequently, these embeddings are input to the network to predict language facets, such as the upcoming word in a sentence or the sentiment of a text.\n",
    "\n",
    "Let's consider the following song lyrics for our analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "song= \"\"\"We are no strangers to love\n",
    "You know the rules and so do I\n",
    "A full commitments what Im thinking of\n",
    "You wouldnt get this from any other guy\n",
    "I just wanna tell you how Im feeling\n",
    "Gotta make you understand\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Weve known each other for so long\n",
    "Your hearts been aching but youre too shy to say it\n",
    "Inside we both know whats been going on\n",
    "We know the game and were gonna play it\n",
    "And if you ask me how Im feeling\n",
    "Dont tell me youre too blind to see\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Weve known each other for so long\n",
    "Your hearts been aching but youre too shy to say it\n",
    "Inside we both know whats been going on\n",
    "We know the game and were gonna play it\n",
    "I just wanna tell you how Im feeling\n",
    "Gotta make you understand\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Tokenization for FNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This PyTorch function is used to obtain a tokenizer for text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens=tokenizer(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_string(s):\n",
    "    \"\"\"\n",
    "    Preprocesses a given string by performing the following steps:\n",
    "    \n",
    "    1. Removes all non-word characters (excluding letters and numbers).\n",
    "    2. Removes all whitespace characters.\n",
    "    3. Removes all numeric digits.\n",
    "\n",
    "    Parameters:\n",
    "    s (str): The input string to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "    str: The processed string with only alphabetic characters, no spaces, and no digits.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove all non-word characters (everything except letters and numbers)\n",
    "    # \\w matches any word character (letters, numbers, and underscores)\n",
    "    # \\s matches any whitespace characters\n",
    "    # ^ inside [] negates the selection, so [^\\w\\s] matches anything that's NOT a word character or whitespace.\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "\n",
    "    # Remove all whitespace characters (spaces, tabs, newlines)\n",
    "    # \\s+ matches one or more whitespace characters.\n",
    "    s = re.sub(r\"\\s+\", '', s)\n",
    "\n",
    "    # Remove all digits (0-9)\n",
    "    # \\d matches any digit character.\n",
    "    s = re.sub(r\"\\d\", '', s)\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(words):\n",
    "    \"\"\"\n",
    "    Preprocesses a given text by tokenizing it, cleaning individual words, and \n",
    "    converting them to lowercase while removing empty or punctuation tokens.\n",
    "\n",
    "    Steps:\n",
    "    1. Tokenization: Splits the input text into individual word tokens.\n",
    "    2. Cleaning: Applies `preprocess_string()` to remove non-word characters, \n",
    "       spaces, and digits from each token.\n",
    "    3. Normalization: Converts all tokens to lowercase.\n",
    "    4. Filtering: Removes empty strings and punctuation tokens.\n",
    "\n",
    "    Parameters:\n",
    "    words (str): The input text to be tokenized and preprocessed.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of cleaned, lowercase tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the input text into words\n",
    "    tokens = word_tokenize(words)\n",
    "\n",
    "    # Apply preprocessing to each token (removes unwanted characters)\n",
    "    tokens = [preprocess_string(w) for w in tokens]\n",
    "\n",
    "    # Convert tokens to lowercase and remove empty strings or punctuation\n",
    "    return [w.lower() for w in tokens if len(w) != 0 and w not in string.punctuation]\n",
    "\n",
    "# Example usage:\n",
    "tokens = preprocess(song)  # Preprocess the text in 'song'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "TorchText provides tools to tokenize text into individual words (tokens) and build a vocabulary, which maps tokens to unique integer indexes. This is a crucial step in preparing text data for  machine learning models that require numerical input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizetext(song):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text (song) and builds a vocabulary from the tokens.\n",
    "\n",
    "    Steps:\n",
    "    1. Tokenization: The function splits the input text into words and applies \n",
    "       a tokenizer function to each word.\n",
    "    2. Vocabulary Building: Constructs a vocabulary from the tokenized words,\n",
    "       including a special \"<unk>\" token to handle out-of-vocabulary words.\n",
    "    3. Default Indexing: Sets the default index for unknown words, ensuring \n",
    "       that any unseen tokens are mapped to \"<unk>\".\n",
    "\n",
    "    Parameters:\n",
    "    song (str): The input text (song lyrics) to be tokenized and processed.\n",
    "\n",
    "    Returns:\n",
    "    vocab (Vocab): A vocabulary object mapping tokens to their corresponding indices.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the text\n",
    "    # Split the input text into words and apply the tokenizer function to each word.\n",
    "    # The 'map' function ensures that each word is tokenized properly.\n",
    "    tokenized_song = map(tokenizer, song.split())\n",
    "\n",
    "    # Build vocabulary from tokenized text\n",
    "    # The function `build_vocab_from_iterator` constructs a vocabulary by iterating \n",
    "    # over the tokenized words. The special token \"<unk>\" is added to handle words \n",
    "    # that are not present in the vocabulary.\n",
    "    vocab = build_vocab_from_iterator(tokenized_song, specials=[\"<unk>\"])\n",
    "\n",
    "    # Set the default index for unknown words\n",
    "    # The default index is set to the index of \"<unk>\" so that any word not found \n",
    "    # in the vocabulary is mapped to this token, preventing errors during lookup.\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the tokens to indices by applying the function as shown here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21, 58, 70, 74, 25, 69, 2, 20, 31, 72]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab=tokenizetext(song)\n",
    "vocab(tokens[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we', 'are', 'no', 'strangers', 'to', 'love', 'you', 'know', 'the', 'rules']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Write a text function that converts raw text into indexes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21, 58, 70, 74, 25, 69, 2, 20, 31, 72]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "text_pipeline(song)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Find the word corresponding to an index using the```get_itos()```method. The result is a list where the index of the list corresponds to a word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'are'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_token = vocab.get_itos()\n",
    "index_to_token[58]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding layers\n",
    "\n",
    "An embedding layer is a crucial element in natural language processing (NLP) and neural networks designed for sequential data. It serves to convert categorical variables, like words or discrete indexes representing tokens, into continuous vectors. This transformation facilitates training and enables the network to learn meaningful relationships among words.\n",
    "\n",
    "Let's consider a simple example involving a vocabulary of words \n",
    "- **Vocabulary**: {apple, banana, orange, pear}\n",
    "\n",
    "Each word in your vocabulary has a unique index assigned to it: \n",
    "- **Indices**: {0, 1, 2, 3}\n",
    "\n",
    "When using an embedding layer, you will initialize random continuous vectors for each index. For instance, the embedding vectors might look like:\n",
    "\n",
    "- Vector for index 0 (apple): [0.2, 0.8]\n",
    "- Vector for index 1 (banana): [0.6, -0.5]\n",
    "- Vector for index 2 (orange): [-0.3, 0.7]\n",
    "- Vector for index 3 (pear): [0.1, 0.4]\n",
    "In PyTorch, you can create an embedding layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genembedding(vocab):\n",
    "    \"\"\"\n",
    "    Generates an embedding layer for the given vocabulary.\n",
    "\n",
    "    The embedding layer transforms words into dense vector representations, \n",
    "    allowing the model to learn semantic relationships between words.\n",
    "\n",
    "    Parameters:\n",
    "    vocab (Vocab): The vocabulary object containing unique words and their indices.\n",
    "\n",
    "    Returns:\n",
    "    nn.Embedding: A PyTorch embedding layer with a specified embedding dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the embedding dimension (size of word vectors)\n",
    "    embedding_dim = 20  # Each word will be represented as a 20-dimensional vector\n",
    "\n",
    "    # Get the vocabulary size (number of unique words in the vocabulary)\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # Create an embedding layer\n",
    "    # The nn.Embedding module maps word indices to dense vector representations.\n",
    "    # It takes vocab_size as the number of words and embedding_dim as the vector size.\n",
    "    embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Embeddings**: Obtain the embedding for the first word with index 0 or 1. Don't forget that you have to convert the input into a tensor. The embeddings are initially initialized randomly, but as the model undergoes training, words with similar meanings gradually come to cluster closer together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word <unk>\n",
      "index 0\n",
      "embedding tensor([-0.4416,  0.1987, -0.5642,  0.3886, -0.4949,  1.8224, -0.9390, -0.9431,\n",
      "        -0.9256, -0.1543, -0.8852,  0.9594, -0.2196,  0.0253, -0.9451, -0.0904,\n",
      "        -0.1765,  1.1066,  1.3127,  0.8478], grad_fn=<EmbeddingBackward0>)\n",
      "embedding shape torch.Size([20])\n",
      "word gonna\n",
      "index 1\n",
      "embedding tensor([ 0.2031, -0.4826,  0.6086, -0.2506, -1.4467, -0.0738,  0.0519, -0.7726,\n",
      "         0.2717,  0.8614, -0.1773,  0.7837,  0.9180,  1.0889, -0.5278,  1.0202,\n",
      "        -0.0848, -0.9176,  1.0847,  2.6091], grad_fn=<EmbeddingBackward0>)\n",
      "embedding shape torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "embeddings=genembedding(vocab)\n",
    "for n in range(2): \n",
    "    embedding=embeddings(torch.tensor(n))\n",
    "    print(\"word\",index_to_token[n])\n",
    "    print(\"index\",n)\n",
    "    print( \"embedding\", embedding)\n",
    "    print(\"embedding shape\", embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These vectors will serve as inputs for the next layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating context-target pairs (n-grams)\n",
    "\n",
    "Organize words within a variable-size context using the following approach: Each word is denoted by 'i'. \n",
    "To establish the context, simply subtract 'j'. The size of the context is determined by the value of``CONTEXT_SIZE``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the context size for generating n-grams\n",
    "CONTEXT_SIZE = 2  # The number of previous words used to predict the next word\n",
    "\n",
    "def genngrams(tokens):\n",
    "    \"\"\"\n",
    "    Generates n-grams from a list of tokens, where each n-gram consists of a \n",
    "    context (previous words) and a target (next word).\n",
    "\n",
    "    The function constructs a list of tuples where:\n",
    "    - The first element is a list of `CONTEXT_SIZE` previous words.\n",
    "    - The second element is the target word that follows the context.\n",
    "\n",
    "    Parameters:\n",
    "    tokens (list): A list of preprocessed word tokens.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of tuples representing n-grams.\n",
    "          Each tuple contains (context_words, target_word).\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate n-grams\n",
    "    # Iterate through the tokens starting from index CONTEXT_SIZE to the end\n",
    "    # For each token at position 'i', extract the previous CONTEXT_SIZE words as context\n",
    "    ngrams = [\n",
    "        (\n",
    "            [tokens[i - j - 1] for j in range(CONTEXT_SIZE)],  # Context words (previous words)\n",
    "            tokens[i]  # Target word (the word to predict)\n",
    "        )\n",
    "        for i in range(CONTEXT_SIZE, len(tokens))\n",
    "    ]\n",
    "\n",
    "    return ngrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the first element, which results in a tuple. The initial element represents the context, and the index indicates the following word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context ['are', 'we'] target no\n",
      "context index [58, 21] target index [70]\n"
     ]
    }
   ],
   "source": [
    "ngrams=genngrams(tokens)\n",
    "context, target=ngrams[0]\n",
    "print(\"context\",context,\"target\",target)\n",
    "print(\"context index\",vocab(context),\"target index\",vocab([target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this context, there are multiple words. Aggregate the embeddings of each of these words and then adjust the input size of the subsequent layer accordingly. Then, create the next layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=20\n",
    "linear = nn.Linear(embedding_dim*CONTEXT_SIZE,128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You have the two embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 20])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings=genembedding(vocab)\n",
    "my_embeddings=embeddings(torch.tensor(vocab(context)))\n",
    "my_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 40])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_embeddings=my_embeddings.reshape(1,-1)\n",
    "my_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "They can now be used as inputs in the next layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6783e-01,  3.1124e-01, -8.7591e-01, -4.0113e-01, -2.6259e-01,\n",
       "          1.0231e-01,  9.0130e-01,  3.8819e-01,  4.2164e-01, -6.4038e-02,\n",
       "         -4.0614e-01, -9.0636e-02,  6.0711e-01, -1.7253e-01,  1.3201e+00,\n",
       "         -3.6939e-01,  7.8297e-01, -1.5414e+00, -1.5098e-01,  3.7475e-01,\n",
       "          2.4093e-01, -7.2883e-01,  2.6184e-01, -3.8409e-01, -5.3202e-01,\n",
       "          2.9983e-01, -2.7491e-02,  3.6961e-01,  5.1479e-01,  8.4731e-01,\n",
       "          6.0853e-01, -3.6576e-02,  1.7182e-01,  5.3580e-01,  6.7128e-01,\n",
       "         -2.4994e-01, -3.4425e-01, -4.5530e-01, -1.5918e-01, -5.2991e-02,\n",
       "         -2.8171e-01, -7.3185e-02,  1.2069e+00, -2.9311e-03,  5.4944e-01,\n",
       "          1.8826e-01, -7.3227e-01, -6.0334e-01, -6.9068e-01,  6.1057e-01,\n",
       "          1.3970e-01,  2.0403e-01, -4.2765e-01, -6.1758e-01,  3.4978e-01,\n",
       "         -9.4502e-01,  3.9437e-01, -5.8735e-01,  2.1682e-01, -6.6225e-01,\n",
       "         -7.4263e-01,  2.3116e-01, -3.7229e-02,  1.0112e+00, -3.6069e-01,\n",
       "          4.2450e-01,  9.9375e-01, -7.6919e-01, -1.3647e-01, -2.2463e-01,\n",
       "          8.9446e-01,  5.9931e-01,  1.6515e-01,  1.9689e-01, -8.8410e-01,\n",
       "         -3.1686e-01,  6.6771e-04,  4.4173e-01,  5.3886e-02, -3.6767e-01,\n",
       "          1.0184e+00, -3.9156e-01, -4.4328e-01,  2.4371e-01,  6.5721e-01,\n",
       "         -4.6086e-01, -9.0398e-02, -4.7341e-01,  5.5444e-01,  8.8704e-02,\n",
       "          7.1661e-01, -1.4508e-01,  7.7344e-01, -8.4256e-01,  3.8657e-01,\n",
       "         -5.7915e-01, -8.9852e-02, -4.2567e-01,  1.7756e-02, -2.9760e-01,\n",
       "          2.9646e-01,  7.4719e-01,  4.0911e-01, -4.2280e-01, -7.0646e-01,\n",
       "          4.0714e-02, -1.9166e-01, -4.3596e-01, -3.7936e-01,  4.4214e-01,\n",
       "          1.0945e+00,  6.4594e-02, -5.6967e-02, -8.7258e-01, -1.8172e-01,\n",
       "         -5.7272e-01, -5.3205e-01, -7.1371e-01,  1.0156e-02, -4.4105e-01,\n",
       "         -1.4003e-01, -5.4755e-01,  1.8263e-01,  6.2311e-01,  2.4852e-01,\n",
       "          3.6740e-01,  1.0908e-01, -4.5856e-01]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(my_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch function\n",
    "\n",
    "Create a Batch function to interface with the data loader. Several adjustments are necessary to handle words that are part of a context in one batch and a predicted word in the following batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader  # Importing DataLoader for batch processing\n",
    "import torch  # Importing PyTorch\n",
    "\n",
    "# Set the device to GPU if available; otherwise, use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define hyperparameters\n",
    "CONTEXT_SIZE = 3   # Number of previous words used as context for prediction\n",
    "BATCH_SIZE = 10    # Number of samples per training batch\n",
    "EMBEDDING_DIM = 10 # Dimension of word embeddings\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\"\n",
    "    Processes a batch of text data into input (context) and output (target) tensors\n",
    "    for training a language model.\n",
    "\n",
    "    The function extracts:\n",
    "    - `context`: A list of word indices representing the context words for each target word.\n",
    "    - `target`: A list of word indices representing the target word to predict.\n",
    "\n",
    "    Parameters:\n",
    "    batch (list): A list of tokenized words (strings).\n",
    "\n",
    "    Returns:\n",
    "    tuple: Two PyTorch tensors: (context_tensor, target_tensor)\n",
    "           - context_tensor: Tensor of shape (batch_size - CONTEXT_SIZE, CONTEXT_SIZE),\n",
    "             containing the word indices of context words.\n",
    "           - target_tensor: Tensor of shape (batch_size - CONTEXT_SIZE,),\n",
    "             containing the word indices of target words.\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = len(batch)  # Get the size of the batch\n",
    "    context, target = [], [] # Initialize lists for context and target words\n",
    "\n",
    "    # Loop through the batch, ensuring enough previous words exist for context\n",
    "    for i in range(CONTEXT_SIZE, batch_size):\n",
    "        # Convert the target word to its index using the vocabulary\n",
    "        target.append(vocab([batch[i]]))\n",
    "\n",
    "        # Convert the previous CONTEXT_SIZE words to indices using the vocabulary\n",
    "        context.append(vocab([batch[i - j - 1] for j in range(CONTEXT_SIZE)]))\n",
    "\n",
    "    # Convert lists to PyTorch tensors and move them to the appropriate device (CPU/GPU)\n",
    "    return torch.tensor(context).to(device), torch.tensor(target).to(device).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, it's important to highlight that the size of the last batch could deviate from that of the earlier batches. To tackle this, the approach involves adjusting the final batch to conform to the specified batch size, ensuring it becomes a multiple of the predetermined size. When necessary, you'll employ padding techniques to achieve this harmonization. One approach you'll use is appending the beginning of the song to the end of the batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Padding=BATCH_SIZE-len(tokens)%BATCH_SIZE\n",
    "tokens_pad=tokens+tokens[0:Padding]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the`DataLoader`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "     tokens_pad, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class neural network\n",
    "\n",
    "You have developed a PyTorch class for a multi-class neural network. The network's output is the probability of the next word within a given context. Therefore, the number of classes corresponds to the count of distinct words. The initial layer consists of embeddings, and in addition to the final layer, an extra hidden layer is incorporated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network-based n-gram language model that predicts the next word \n",
    "    given a sequence of context words.\n",
    "\n",
    "    This model consists of:\n",
    "    - An embedding layer that converts word indices into dense vector representations.\n",
    "    - A fully connected hidden layer with ReLU activation.\n",
    "    - An output layer that predicts the probability distribution over the vocabulary.\n",
    "\n",
    "    Parameters:\n",
    "    vocab_size (int): The number of unique words in the vocabulary.\n",
    "    embedding_dim (int): The size of the word embeddings (vector representation of words).\n",
    "    context_size (int): The number of previous words used as context to predict the next word.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "\n",
    "        # Store context size and embedding dimension\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Embedding layer: Maps word indices to dense vectors\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Fully connected hidden layer: Maps the concatenated embeddings to a 128-dimensional space\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "\n",
    "        # Output layer: Maps the hidden layer output to vocabulary size (probability distribution over words)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        inputs (Tensor): A tensor of shape (batch_size, context_size) containing word indices.\n",
    "\n",
    "        Returns:\n",
    "        Tensor: A tensor of shape (batch_size, vocab_size) representing predicted probabilities for the next word.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert input word indices into dense vectors using the embedding layer\n",
    "        embeds = self.embeddings(inputs)  # Shape: (batch_size, context_size, embedding_dim)\n",
    "\n",
    "        # Reshape the embeddings into a single vector per input sample\n",
    "        embeds = torch.reshape(embeds, (-1, self.context_size * self.embedding_dim))  \n",
    "        # New shape: (batch_size, context_size * embedding_dim)\n",
    "\n",
    "        # Apply first fully connected layer with ReLU activation\n",
    "        out = F.relu(self.linear1(embeds))  # Shape: (batch_size, 128)\n",
    "\n",
    "        # Apply second fully connected layer to generate vocabulary-size logits\n",
    "        out = self.linear2(out)  # Shape: (batch_size, vocab_size)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve samples from the data loader object and input them into the neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[70, 58, 21],\n",
      "        [74, 70, 58],\n",
      "        [25, 74, 70],\n",
      "        [69, 25, 74],\n",
      "        [ 2, 69, 25],\n",
      "        [20,  2, 69],\n",
      "        [31, 20,  2]]) tensor([74, 25, 69,  2, 20, 31, 72])\n"
     ]
    }
   ],
   "source": [
    "context, target=next(iter(dataloader))\n",
    "print(context, target)\n",
    "out=model(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the model remains untrained, analyzing the output can provide us with a clearer understanding. In the output, the first dimension corresponds to the batch size, while the second dimension represents the probability associated with each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 79])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the index with the highest probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30,  9, 31, 53, 58, 61, 41])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_index =torch.argmax(out,1)\n",
    "predicted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the corresponding token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['so', 'around', 'the', 'were', 'are', 'commitments', 'gotta']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[index_to_token[i.item()] for i in  predicted_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that accomplishes the same task for the tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_song(model, my_song, number_of_words=100):\n",
    "    \"\"\"\n",
    "    Generates text using a trained n-gram language model.\n",
    "\n",
    "    Given an initial text (`my_song`), the function generates additional words by \n",
    "    predicting the next word iteratively based on the trained model.\n",
    "\n",
    "    Parameters:\n",
    "    model (nn.Module): The trained n-gram language model.\n",
    "    my_song (str): The initial seed text to start generating words.\n",
    "    number_of_words (int): The number of words to generate (default: 100).\n",
    "\n",
    "    Returns:\n",
    "    str: The generated song lyrics as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the mapping from index to word for decoding predictions\n",
    "    index_to_token = vocab.get_itos()\n",
    "\n",
    "    # Loop to generate the desired number of words\n",
    "    for i in range(number_of_words):\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation for inference\n",
    "            \n",
    "            # Prepare the input context by extracting the last CONTEXT_SIZE words from tokens\n",
    "            context = torch.tensor(\n",
    "                vocab([tokens[i - j - 1] for j in range(CONTEXT_SIZE)])\n",
    "            ).to(device)  # Move to CPU/GPU as required\n",
    "            \n",
    "            # Predict the next word by selecting the word with the highest probability\n",
    "            word_idx = torch.argmax(model(context))  # Get index of the most likely next word\n",
    "            \n",
    "            # Append the predicted word to the generated text\n",
    "            my_song += \" \" + index_to_token[word_idx.detach().item()]\n",
    "\n",
    "    return my_song  # Return the generated lyrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Never gonna run around and desert you understand aching are so around the were are commitments gotta understand commitments wanna know understand are commitments play game so thinking been game commitments commitments your thinking understand thinking aching wanna hearts are lie are are commitments so thinking how full commitments wanna are so are the commitments do ask hearts thinking the aching are understand so thinking the wanna understand are understand understand aching commitments the commitments wanna commitments so are the were thinking are thinking the are are commitments wanna commitments understand thinking commitments so tell aching wanna commitments understand ask commitments the understand ask been your are\n"
     ]
    }
   ],
   "source": [
    "def pickrandomline(song):\n",
    "    \"\"\"\n",
    "    Selects a random line from the given song text.\n",
    "\n",
    "    This function splits the song into separate lines and randomly picks one of them.\n",
    "\n",
    "    Parameters:\n",
    "    song (str): The song lyrics as a multi-line string.\n",
    "\n",
    "    Returns:\n",
    "    str: A randomly selected line from the song.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split the song into individual lines\n",
    "    lines = song.split(\"\\n\")  \n",
    "    \n",
    "    # Randomly select a line and remove leading/trailing whitespace\n",
    "    selected_line = random.choice(lines).strip()\n",
    "    \n",
    "    return selected_line  # Return the randomly selected line\n",
    "\n",
    "# Example usage:\n",
    "selected_line = pickrandomline(song)  # Pick a random line from the song\n",
    "\n",
    "# Generate a new song starting with the selected line\n",
    "generated_song = write_song(model, selected_line)\n",
    "\n",
    "# Print the generated lyrics\n",
    "print(generated_song)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Training a language model involves a multi-step process that leverages training and testing data to optimize model performance. In the realm of Natural Language Processing (NLP), this process often employs various metrics to gauge a model's accuracy, such as perplexity or accuracy on unseen data. However, in the context of your current exploration, you will embark on a slightly different journey. Instead of relying solely on conventional NLP metrics, the focus shifts to manual inspection of the results. \n",
    "\n",
    "You have the cross entropy loss between input logits and target:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have developed a function dedicated to training the model using the supplied data loader. In addition to training the model, the function's output includes predictions for each epoch, spanning context for the next 100 words.\n",
    "\n",
    "\n",
    ">**Note: Since the dataset used is relatively small and the model is trained for a limited number of epochs, the generated song may not always be fully coherent due to a restricted vocabulary. This example primarily serves to demonstrate the functionality of a feedforward neural network for language modeling.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model,song,number_of_epochs=100, show=10):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataloader (DataLoader): DataLoader containing training data.\n",
    "        model (nn.Module): Neural network model to be trained.\n",
    "        number_of_epochs (int, optional): Number of epochs for training. Default is 100.\n",
    "        show (int, optional): Interval for displaying progress. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "        list: List containing loss values for each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    MY_LOSS = []  # List to store loss values for each epoch\n",
    "\n",
    "    # Iterate over the specified number of epochs\n",
    "    for epoch in tqdm(range(number_of_epochs)):\n",
    "        total_loss = 0  # Initialize total loss for the current epoch\n",
    "        my_song = \"\"    # Initialize a string to store the generated song\n",
    "\n",
    "        # Iterate over batches in the dataloader\n",
    "        for context, target in dataloader:\n",
    "            model.zero_grad()          # Zero the gradients to avoid accumulation\n",
    "            predicted = model(context)  # Forward pass through the model to get predictions\n",
    "            loss = criterion(predicted, target.reshape(-1))  # Calculate the loss\n",
    "            total_loss += loss.item()   # Accumulate the loss\n",
    "\n",
    "            loss.backward()    # Backpropagation to compute gradients\n",
    "            optimizer.step()   # Update model parameters using the optimizer\n",
    "\n",
    "        # Display progress and generate song at specified intervals\n",
    "        if epoch % show == 0:\n",
    "            selected_line=pickrandomline(song)\n",
    "            my_song += write_song(model, selected_line)    # Generate song using the model\n",
    "\n",
    "            print(\"Generated Song:\")\n",
    "            print(\"\\n\")\n",
    "            print(my_song)\n",
    "\n",
    "        MY_LOSS.append(total_loss/len(dataloader))  # Append the total loss for the epoch to MY_LOSS list\n",
    "\n",
    "    return MY_LOSS  # Return the list of  mean loss values for each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following list will be used to store the loss for each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_loss_list=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code segment initializes an n-gram language model with a context size of 2. The model, named `model_2`, is configured based on the provided vocabulary size, embedding dimension, and context size. The Stochastic Gradient Descent (SGD) optimizer is employed with a learning rate of 0.01 to manage model parameter updates. A learning rate scheduler, using a step-wise approach with a reduction factor of 0.1 per epoch, is set up to adapt the learning rate during the training process. These settings collectively establish the framework for training the n-gram language model with tailored optimization and learning rate adjustment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the context size for the n-gram model\n",
    "CONTEXT_SIZE = 2\n",
    "\n",
    "# Create an instance of the NGramLanguageModeler class with specified parameters\n",
    "model_2 = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE).to(device)\n",
    "\n",
    "# Define the optimizer for training the model, using stochastic gradient descent (SGD)\n",
    "optimizer = optim.SGD(model_2.parameters(), lr=0.01)\n",
    "\n",
    "# Set up a learning rate scheduler using StepLR to adjust the learning rate during training\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1.0, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:00<00:15,  6.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Song:\n",
      "\n",
      "\n",
      "Never gonna make you cry weve let what <unk> <unk> <unk> <unk> up let <unk> <unk> inside <unk> <unk> <unk> <unk> never <unk> <unk> <unk> <unk> <unk> for <unk> <unk> <unk> <unk> <unk> do <unk> <unk> this <unk> <unk> <unk> up let <unk> <unk> <unk> <unk> you for never <unk> <unk> <unk> <unk> were never <unk> <unk> <unk> <unk> we let give <unk> <unk> <unk> commitments never let for never <unk> <unk> you for let <unk> <unk> <unk> up we <unk> <unk> <unk> <unk> up <unk> up you weve let <unk> <unk> weve <unk> never <unk> <unk> <unk> strangers <unk> <unk> <unk> <unk> <unk> <unk>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [00:02<00:17,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Song:\n",
      "\n",
      "\n",
      "Never gonna let you down never never <unk> <unk> <unk> <unk> <unk> never <unk> you <unk> <unk> <unk> you <unk> <unk> <unk> <unk> <unk> <unk> you <unk> never <unk> <unk> <unk> <unk> <unk> you <unk> you <unk> <unk> you you never <unk> <unk> <unk> <unk> you you never never <unk> <unk> you you never never <unk> <unk> you you never never <unk> <unk> you you <unk> never you never <unk> <unk> you you never never <unk> <unk> you you never <unk> <unk> you you <unk> <unk> <unk> you never never <unk> <unk> <unk> you <unk> <unk> <unk> and <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [00:03<00:12,  6.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Song:\n",
      "\n",
      "\n",
      "You wouldnt get this from any other guy never never <unk> <unk> to never never down never you <unk> and <unk> you <unk> you never <unk> <unk> <unk> you and never <unk> <unk> <unk> <unk> <unk> you <unk> you <unk> <unk> tell you never im <unk> <unk> <unk> tell you never never <unk> <unk> tell you never never <unk> <unk> tell you down never <unk> <unk> tell you and and you never <unk> <unk> tell you never never <unk> <unk> tell goodbye never <unk> <unk> tell you you and hurt you never never down <unk> <unk> you <unk> you and and <unk> <unk> <unk> and you <unk> you\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [00:05<00:10,  6.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Song:\n",
      "\n",
      "\n",
      "Never gonna let you down never never <unk> <unk> to im and know never you and hurt <unk> you im you never <unk> <unk> <unk> you and never <unk> and <unk> <unk> and <unk> <unk> you <unk> <unk> tell you never im feeling <unk> <unk> tell you understand never <unk> <unk> tell you down never <unk> <unk> tell you down never <unk> <unk> tell you and desert you never <unk> <unk> tell you understand never <unk> <unk> tell goodbye never <unk> <unk> tell you goodbye and hurt you never never each <unk> <unk> you <unk> you and been <unk> <unk> <unk> and you <unk> you\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [00:07<00:09,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Song:\n",
      "\n",
      "\n",
      "Never gonna run around and desert you never never <unk> <unk> to im and know never you and know <unk> you im you never <unk> <unk> <unk> you hurt never <unk> been <unk> <unk> and <unk> <unk> you <unk> <unk> tell you how im feeling <unk> <unk> tell you understand never <unk> <unk> tell you down never <unk> <unk> tell you down never <unk> <unk> tell you and desert you never <unk> <unk> tell you understand never <unk> <unk> tell goodbye never <unk> <unk> tell you goodbye and hurt you never known each <unk> for you <unk> you and been aching but youre and shy never im\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [00:09<00:08,  5.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Song:\n",
      "\n",
      "\n",
      "Dont tell me youre too blind to see never never no strangers to im and know never you and know <unk> you a you never <unk> <unk> <unk> you hurt never <unk> been been <unk> and <unk> <unk> tell <unk> <unk> tell you how im feeling <unk> <unk> tell you understand never <unk> <unk> tell you down never <unk> <unk> tell you down never <unk> <unk> tell you and desert you never <unk> <unk> tell you understand never <unk> <unk> tell goodbye never <unk> <unk> tell you goodbye and hurt you never known each other for you <unk> you and been aching but youre and shy we im\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [00:10<00:05,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Song:\n",
      "\n",
      "\n",
      "Never gonna tell a lie and hurt you never never no strangers to im and know never game and know <unk> you a you never <unk> <unk> <unk> you hurt never <unk> been been <unk> been <unk> i a <unk> <unk> tell you how im feeling <unk> <unk> tell you understand never <unk> <unk> tell you know never <unk> <unk> tell you down never <unk> <unk> tell you and desert you never <unk> <unk> tell you understand never <unk> <unk> tell goodbye never <unk> <unk> tell you how and hurt you never known each other for you never you and been aching but youre too shy we im\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72/100 [00:12<00:04,  5.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Song:\n",
      "\n",
      "\n",
      "Your hearts been aching but youre too shy to say it never never no strangers to youre and know never game and know <unk> you a you never <unk> <unk> <unk> you hurt never <unk> been been <unk> other <unk> i a <unk> <unk> tell you how im feeling <unk> <unk> tell you understand never <unk> <unk> tell you know never <unk> <unk> tell you down never <unk> <unk> tell you and desert you never <unk> <unk> tell you understand never <unk> <unk> tell goodbye never <unk> <unk> tell you lie and hurt you never known each other for you never you and been aching but youre too shy we im\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [00:13<00:02,  6.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Song:\n",
      "\n",
      "\n",
      "We are no strangers to love never never no strangers to love it know the game and know <unk> you a full never you <unk> <unk> you hurt never <unk> this from <unk> other guy i a <unk> <unk> tell you how im feeling <unk> <unk> tell you understand never <unk> <unk> tell you up never <unk> <unk> tell you down never <unk> <unk> tell you and desert you never <unk> <unk> tell you understand never <unk> <unk> tell goodbye never <unk> <unk> tell you lie and hurt you never known each other for you we you and been aching but youre too shy we im\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [00:15<00:01,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Song:\n",
      "\n",
      "\n",
      "Weve known each other for so long never are no strangers to love you know the game and know <unk> i a full never you <unk> thinking <unk> hurt never <unk> this from <unk> other guy i a <unk> <unk> tell you how im feeling <unk> <unk> tell you understand never <unk> <unk> tell you up never <unk> <unk> tell you down never <unk> <unk> tell you and desert you never <unk> <unk> tell you understand never <unk> <unk> tell goodbye never <unk> <unk> tell you lie and hurt you never known each other for so we you and been aching but youre too shy we im\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:16<00:00,  5.96it/s]\n"
     ]
    }
   ],
   "source": [
    "my_loss=train(dataloader,model_2,song)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '2gram.pth'\n",
    "torch.save(model_2.state_dict(), save_path)\n",
    "my_loss_list.append(my_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code provided below shows word embeddings from the created model, reduces their dimensionality to 2D using t-SNE, and then plots them as a scatter plot. Additionally, it annotates the first 20 points in the visualization with their corresponding words. This is used to visualize how similar words cluster together in a lower-dimensional space, revealing the structure of the word embeddings. Embeddings allow the model to represent words in a continuous vector space, capturing semantic relationships and similarities between words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m tsne \u001b[38;5;241m=\u001b[39m TSNE(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      3\u001b[0m X_2d \u001b[38;5;241m=\u001b[39m tsne\u001b[38;5;241m.\u001b[39mfit_transform(X)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "X = model_2.embeddings.weight.cpu().detach().numpy()\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_2d = tsne.fit_transform(X)\n",
    "\n",
    "labels = []\n",
    "\n",
    "for j in range(len(X_2d)):\n",
    "    if j < 20:\n",
    "        plt.scatter(X_2d[j, 0], X_2d[j, 1], label=index_to_token[j])\n",
    "        labels.append(index_to_token[j])\n",
    "        # Add words as annotations\n",
    "        plt.annotate(index_to_token[j],\n",
    "                     (X_2d[j, 0], X_2d[j, 1]),\n",
    "                     textcoords=\"offset points\",\n",
    "                     xytext=(0, 10),\n",
    "                     ha='center')\n",
    "    else:\n",
    "        plt.scatter(X_2d[j, 0], X_2d[j, 1])\n",
    "\n",
    "plt.legend(labels, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the process for a context of four.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:00<00:13,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Song:\n",
      "\n",
      "\n",
      "Gotta make you understand guy you what <unk> on shy were <unk> you shy <unk> <unk> on make you so <unk> <unk> you <unk> thinking <unk> you thinking you known <unk> make <unk> <unk> <unk> <unk> <unk> <unk> on never you you <unk> <unk> <unk> <unk> you you you <unk> up <unk> you run gotta <unk> im you you im im <unk> <unk> know never you <unk> <unk> <unk> <unk> <unk> you you never <unk> <unk> <unk> on you you <unk> tell on you run <unk> you guy <unk> you gotta im up im on you <unk> you <unk> are you <unk> <unk> tell\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [00:02<00:15,  5.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Song:\n",
      "\n",
      "\n",
      "Never gonna make you cry never <unk> <unk> <unk> <unk> never <unk> <unk> <unk> <unk> never <unk> <unk> <unk> <unk> <unk> <unk> <unk> you <unk> <unk> never you never <unk> <unk> <unk> never <unk> <unk> <unk> <unk> <unk> <unk> you never never <unk> never <unk> <unk> you never never <unk> <unk> tell you you never <unk> <unk> never you never never <unk> <unk> <unk> you never never you never <unk> <unk> <unk> you never never <unk> <unk> <unk> you never <unk> <unk> tell you <unk> <unk> <unk> you never <unk> you <unk> <unk> <unk> <unk> never <unk> <unk> <unk> <unk> <unk> never <unk> <unk> <unk>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [00:03<00:12,  6.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Song:\n",
      "\n",
      "\n",
      "Never gonna let you down never <unk> <unk> <unk> you never you never never <unk> never <unk> you never <unk> <unk> <unk> <unk> never <unk> <unk> never you never <unk> <unk> <unk> never <unk> <unk> <unk> you <unk> tell you never never <unk> never <unk> say you down never <unk> <unk> give you down never <unk> <unk> let you down never <unk> <unk> make you never never you never <unk> <unk> make you down never <unk> <unk> say you never <unk> <unk> tell you <unk> <unk> never you never <unk> you <unk> <unk> <unk> never never <unk> been you <unk> youre never <unk> never <unk>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [00:05<00:11,  5.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Song:\n",
      "\n",
      "\n",
      "Weve known each other for so long never known <unk> <unk> to never you know never <unk> never <unk> you never <unk> <unk> <unk> <unk> im <unk> <unk> never you never <unk> <unk> <unk> never tell <unk> never you <unk> tell you down never <unk> never <unk> say you down never <unk> <unk> give you down never <unk> <unk> let you down never <unk> <unk> make you never never you never <unk> <unk> make you down never <unk> <unk> say goodbye never <unk> <unk> tell goodbye <unk> <unk> never you never <unk> each other for <unk> never to <unk> been you <unk> youre too <unk> never <unk>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [00:07<00:08,  6.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Song:\n",
      "\n",
      "\n",
      "I just wanna tell you how Im feeling never known know <unk> to never you know im <unk> never <unk> you never <unk> <unk> <unk> what im know <unk> never you never been <unk> for other tell <unk> never you <unk> tell you how im feeling <unk> <unk> say you down never <unk> <unk> give you down never <unk> <unk> let you down never <unk> <unk> give you never never you never <unk> <unk> make you down never <unk> <unk> say goodbye never <unk> <unk> tell goodbye <unk> im never you never <unk> each other for <unk> never to <unk> been you but youre too shy to <unk>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [00:08<00:07,  6.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Song:\n",
      "\n",
      "\n",
      "Never gonna make you cry never known know strangers to love you know im <unk> im <unk> you never <unk> <unk> <unk> what im thinking <unk> never we never been other for other tell <unk> never you <unk> tell you how im feeling <unk> <unk> say you down never <unk> <unk> give you down never <unk> <unk> let you down never <unk> <unk> give you how never you never <unk> <unk> make you down never <unk> <unk> say goodbye never <unk> <unk> tell goodbye lie im were you never known each other for other never we <unk> been aching but youre too shy to say\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [00:10<00:05,  6.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Song:\n",
      "\n",
      "\n",
      "Never gonna say goodbye never known no strangers to love you know the <unk> im <unk> you never a full commitments what im thinking <unk> never we never this other for other guy <unk> never we <unk> tell you how im feeling <unk> <unk> say you down never <unk> <unk> give you down never <unk> <unk> let you down never <unk> <unk> give around how never you never <unk> <unk> make you down never <unk> <unk> say goodbye never <unk> <unk> tell goodbye lie im were you never known each other for so long we <unk> been aching but youre too shy to say\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72/100 [00:11<00:04,  6.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Song:\n",
      "\n",
      "\n",
      "We know the game and were gonna play it never are no strangers to love you know the <unk> im were you never a full commitments what im thinking <unk> never we never this other any other guy i never we <unk> tell you how im feeling <unk> <unk> say you down never <unk> <unk> give you down never <unk> <unk> let you down never <unk> <unk> run around how never you never <unk> <unk> make you down never <unk> <unk> say goodbye never <unk> <unk> tell goodbye lie im were you never known each other for so long we <unk> been aching but youre too shy to say\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [00:13<00:02,  6.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Song:\n",
      "\n",
      "\n",
      "Never gonna give you up never are no strangers to love you know the rules im were you never a full commitments what im thinking <unk> never we never this from any other guy i never we <unk> tell you how im feeling <unk> <unk> say you down never <unk> <unk> give you up never <unk> <unk> let you down never <unk> <unk> run around how never you never <unk> <unk> make you down never <unk> <unk> say goodbye never <unk> <unk> tell goodbye lie im were you never known each other for so long we <unk> been aching but youre too shy to say\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [00:15<00:01,  6.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Song:\n",
      "\n",
      "\n",
      "Never gonna let you down never are no strangers to love you know the rules im were you never a full commitments what im thinking <unk> a we never this from any other guy i never we <unk> tell you how im feeling <unk> <unk> say you down never <unk> <unk> give you up never <unk> <unk> let you down never <unk> <unk> run around how never you never <unk> <unk> make you down never <unk> <unk> say goodbye never <unk> <unk> tell goodbye lie im were you never known each other for so long we <unk> been aching but youre too shy to say\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:16<00:00,  6.11it/s]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE=4\n",
    "model_4 = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE).to(device)\n",
    "optimizer = optim.SGD(model_4.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "my_loss=train(dataloader,model_4,song)\n",
    "\n",
    "save_path = '4gram.pth'\n",
    "torch.save(model_4.state_dict(), save_path)\n",
    "\n",
    "my_loss_list.append(my_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code provided below shows word embeddings from the created model, reduces their dimensionality to 2d using t-SNE, and then plots them as a scatter plot. Additionally, it annotates the first 20 points in the visualization with their corresponding words. This is used to visualize how similar words cluster together in a lower-dimensional space, revealing the structure of the word embeddings. Embeddings allow the model to represent words in a continuous vector space, capturing semantic relationships and similarities between words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_4\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m tsne \u001b[38;5;241m=\u001b[39m TSNE(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      3\u001b[0m X_2d \u001b[38;5;241m=\u001b[39m tsne\u001b[38;5;241m.\u001b[39mfit_transform(X)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "X = model_4.embeddings.weight.cpu().detach().numpy()\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_2d = tsne.fit_transform(X)\n",
    "\n",
    "labels = []\n",
    "\n",
    "for j in range(len(X_2d)):\n",
    "    if j < 20:\n",
    "        plt.scatter(X_2d[j, 0], X_2d[j, 1], label=index_to_token[j])\n",
    "        labels.append(index_to_token[j])\n",
    "        # Add words as annotations\n",
    "        plt.annotate(index_to_token[j],\n",
    "                     (X_2d[j, 0], X_2d[j, 1]),\n",
    "                     textcoords=\"offset points\",\n",
    "                     xytext=(0, 10),\n",
    "                     ha='center')\n",
    "    else:\n",
    "        plt.scatter(X_2d[j, 0], X_2d[j, 1])\n",
    "\n",
    "plt.legend(labels, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for a context of eight.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:00<00:19,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Song:\n",
      "\n",
      "\n",
      "Never gonna run around and desert you <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> thinking <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> you <unk> <unk> <unk> <unk> <unk> <unk> <unk> you <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> make <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> you <unk> <unk> <unk> <unk>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [00:01<00:11,  7.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Song:\n",
      "\n",
      "\n",
      "Never gonna let you down <unk> <unk> <unk> give to never <unk> <unk> the <unk> <unk> <unk> tell <unk> tell make <unk> you im <unk> <unk> tell <unk> <unk> <unk> you make you you i <unk> <unk> <unk> give you tell <unk> <unk> <unk> <unk> give you you never <unk> <unk> give you you never <unk> <unk> give you you never <unk> <unk> give you you never <unk> <unk> <unk> <unk> make you you never <unk> <unk> give you you never <unk> tell you lie never <unk> <unk> <unk> <unk> <unk> give you so never <unk> <unk> <unk> <unk> make you too tell to say\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [00:03<00:11,  6.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Song:\n",
      "\n",
      "\n",
      "Dont tell me youre too blind to see <unk> <unk> give strangers to cry im <unk> the rules tell <unk> been <unk> tell it <unk> im im thinking <unk> tell <unk> <unk> strangers you make give guy i im to <unk> give you lie im feeling <unk> <unk> give you you never <unk> <unk> give you up never <unk> <unk> give you up never <unk> <unk> run around you never <unk> <unk> <unk> <unk> make you cry never <unk> <unk> give you up never <unk> tell a lie never <unk> <unk> <unk> <unk> <unk> other for so long im <unk> <unk> <unk> never im too shy to say\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [00:04<00:10,  6.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Song:\n",
      "\n",
      "\n",
      "Never gonna tell a lie and hurt you <unk> <unk> give strangers to cry im <unk> the rules tell going been <unk> tell it <unk> i im thinking <unk> tell <unk> <unk> strangers you make give guy i im to <unk> give you up im feeling <unk> <unk> give you up never <unk> <unk> give you up never <unk> <unk> give you up never <unk> <unk> run around up never <unk> <unk> <unk> <unk> make you cry never <unk> <unk> give you up never <unk> tell a lie never <unk> tell <unk> im <unk> other for so long im <unk> <unk> <unk> so im too shy to say\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [00:06<00:07,  7.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Song:\n",
      "\n",
      "\n",
      "Never gonna run around and desert you <unk> <unk> give strangers to cry im <unk> the rules tell going been <unk> tell it <unk> i im thinking <unk> tell <unk> <unk> strangers for make give guy i im to <unk> give you up im feeling <unk> <unk> give you up never <unk> <unk> give you up never <unk> <unk> give you up never <unk> <unk> run around up never <unk> <unk> <unk> <unk> make you cry never <unk> <unk> give you up never <unk> tell a lie never <unk> tell <unk> im strangers other for so long im <unk> <unk> <unk> so im too shy to say\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [00:07<00:07,  6.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Song:\n",
      "\n",
      "\n",
      "Never gonna run around and desert you <unk> <unk> give strangers to cry im <unk> the rules feeling going been <unk> tell it <unk> i im thinking <unk> tell <unk> <unk> strangers for make give guy i im to <unk> give you up im feeling <unk> <unk> give you up never <unk> <unk> give you up never <unk> <unk> give you up never <unk> <unk> run around up never <unk> <unk> <unk> <unk> make you cry never <unk> <unk> give you up never <unk> tell a lie never <unk> tell <unk> im strangers other for so long im <unk> <unk> <unk> so im too shy to say\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [00:09<00:05,  6.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Song:\n",
      "\n",
      "\n",
      "Never gonna run around and desert you <unk> <unk> give strangers to cry im <unk> the rules feeling going been <unk> tell it other i im thinking <unk> tell <unk> <unk> strangers for make give guy i im to <unk> give you up im feeling <unk> <unk> give you up never <unk> <unk> give you up never <unk> <unk> give you up never <unk> <unk> run around up never <unk> <unk> <unk> <unk> make you cry never <unk> <unk> give you up never <unk> tell a lie never <unk> tell <unk> im strangers other for so long never <unk> <unk> <unk> on im too shy to say\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72/100 [00:10<00:04,  6.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Song:\n",
      "\n",
      "\n",
      "Never gonna run around and desert you <unk> <unk> give strangers to cry im <unk> the rules feeling going been <unk> tell it other i im thinking <unk> tell <unk> <unk> strangers for make give guy i im to <unk> give you up im feeling <unk> <unk> give you up never <unk> <unk> give you up never <unk> <unk> give you up never <unk> <unk> run around up never <unk> <unk> <unk> <unk> make you cry never <unk> <unk> give you up never <unk> tell a lie never <unk> tell <unk> im strangers other for so long never <unk> <unk> <unk> on im too shy to say\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [00:12<00:02,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Song:\n",
      "\n",
      "\n",
      "I just wanna tell you how Im feeling <unk> <unk> give strangers to cry im <unk> the rules feeling going been <unk> tell it other i im thinking <unk> tell <unk> <unk> other for make give guy i im to <unk> give you up im feeling <unk> <unk> give you up never <unk> <unk> give you up never <unk> <unk> give you up never <unk> <unk> run around up never <unk> <unk> <unk> <unk> make you cry never <unk> <unk> give you up never <unk> tell a lie never <unk> tell <unk> im strangers other for so long never <unk> <unk> <unk> on im too shy to say\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [00:13<00:01,  6.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Song:\n",
      "\n",
      "\n",
      "Never gonna give you up <unk> <unk> give strangers to cry im <unk> the rules feeling going been <unk> tell it other i im thinking <unk> tell <unk> <unk> other for make give guy i im to <unk> give you up im feeling <unk> <unk> give you up never <unk> <unk> give you up never <unk> <unk> give you up never <unk> <unk> run around up never <unk> <unk> <unk> <unk> make you cry never <unk> <unk> give you up never <unk> tell a lie never <unk> tell <unk> <unk> strangers other for so long never <unk> <unk> <unk> on im too shy to say\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:14<00:00,  6.68it/s]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE=8\n",
    "model_8 = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE).to(device)\n",
    "optimizer = optim.SGD(model_8.parameters(), lr=0.01)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "my_loss=train(dataloader,model_8,song)\n",
    "\n",
    "save_path = '8gram.pth'\n",
    "torch.save(model_8.state_dict(), save_path)\n",
    "\n",
    "my_loss_list.append(my_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code provided below shows word embeddings from the created model, reduces their dimensionality to 2D using t-SNE, and then plots them as a scatter plot. Additionally, it annotates the first 20 points in the visualization with their corresponding words. This is used to visualize how similar words cluster together in a lower-dimensional space, revealing the structure of the word embeddings. Embeddings allow the model to represent words in a continuous vector space, capturing semantic relationships and similarities between words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_8\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m tsne \u001b[38;5;241m=\u001b[39m TSNE(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      3\u001b[0m X_2d \u001b[38;5;241m=\u001b[39m tsne\u001b[38;5;241m.\u001b[39mfit_transform(X)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "X = model_8.embeddings.weight.cpu().detach().numpy()\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_2d = tsne.fit_transform(X)\n",
    "\n",
    "labels = []\n",
    "\n",
    "for j in range(len(X_2d)):\n",
    "    if j < 20:\n",
    "        plt.scatter(X_2d[j, 0], X_2d[j, 1], label=index_to_token[j])\n",
    "        labels.append(index_to_token[j])\n",
    "        # Add words as annotations\n",
    "        plt.annotate(index_to_token[j],\n",
    "                     (X_2d[j, 0], X_2d[j, 1]),\n",
    "                     textcoords=\"offset points\",\n",
    "                     xytext=(0, 10),\n",
    "                     ha='center')\n",
    "    else:\n",
    "        plt.scatter(X_2d[j, 0], X_2d[j, 1])\n",
    "\n",
    "plt.legend(labels, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When considering the plotted loss for each model, a discernible trend emerges: an increase in context size correlates with a reduction in loss. While this specific approach lacks the inclusion of model validation or the utilization of conventional NLP evaluation metrics, the visual evidence substantiates its superior performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for (my_loss, model_name)in zip(my_loss_list,[\"2-gram\",\"4-gram\",\"8-gram\"]):\n",
    "    plt.plot(my_loss,label=\"Cross-entropy Loss - {}\".format(model_name))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity\n",
    "Perplexity is a measurement used to evaluate the effectiveness of language models or probability models. It provides an indication of how well a model predicts a sample of data or the likelihood of an unseen event. Perplexity is commonly used in natural language processing tasks, such as machine translation, speech recognition, and language generation.\n",
    "\n",
    "Perplexity is derived from the concept of cross-entropy loss, which measures the dissimilarity between predicted probabilities and actual probabilities. \n",
    "\n",
    "$$\\text{Cross-Entropy Loss} = -\\sum_{i=1}^{N} y_i \\ln(p_i)$$\n",
    "The cross-entropy loss is calculated by taking the negative sum of the products of the true labels $y_i$ and the logarithm of the predicted probabilities $p_i$ over $N$ classes.\n",
    "\n",
    "Taking the exponential of the mean cross-entropy loss gives us the perplexity value.\n",
    "\n",
    "$$\\text{Perplexity} = e^{\\frac{1}{N} \\text{Cross-Entropy Loss}}$$\n",
    "\n",
    "\n",
    "A lower perplexity value indicates that the model is more confident and accurate in predicting the data. Conversely, a higher perplexity suggests that the model is less certain and less accurate in its predictions.\n",
    "\n",
    "Perplexity can be seen as an estimate of the average number of choices the model has for the next word or event in a sequence. A lower perplexity means that the model is more certain about the next word, while a higher perplexity means that there are more possible choices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (my_loss, model_name)in zip(my_loss_list,[\"2-gram\",\"4-gram\",\"8-gram\"]):\n",
    "    # Calculate perplexity using the loss\n",
    "    perplexity = np.exp(my_loss)\n",
    "    plt.plot(perplexity,label=\"Perplexity - {}\".format(model_name))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Source a collection of nursery rhymes and compile them into a single text variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "nursery_rhymes = \"\"\"\n",
    "Little Miss Muffet\n",
    "Sat on a tuffet,\n",
    "Eating her curds and whey;\n",
    "Along came a spider\n",
    "Who sat down beside her\n",
    "And frightened Miss Muffet away.\n",
    "\n",
    "Twinkle, twinkle, little star,\n",
    "How I wonder what you are!\n",
    "Up above the world so high,\n",
    "Like a diamond in the sky.\n",
    "\n",
    "Baa, baa, black sheep,\n",
    "Have you any wool?\n",
    "Yes sir, yes sir,\n",
    "Three bags full.\n",
    "\n",
    "Jack and Jill went up the hill\n",
    "To fetch a pail of water.\n",
    "Jack fell down and broke his crown,\n",
    "And Jill came tumbling after.\n",
    "\n",
    "Hickory dickory dock,\n",
    "The mouse ran up the clock.\n",
    "The clock struck one,\n",
    "The mouse ran down,\n",
    "Hickory dickory dock.\n",
    "\n",
    "Humpty Dumpty sat on a wall,\n",
    "Humpty Dumpty had a great fall.\n",
    "All the king's horses and all the king's men\n",
    "Couldn't put Humpty together again.\n",
    "\n",
    "Mary had a little lamb,\n",
    "Its fleece was white as snow;\n",
    "And everywhere that Mary went,\n",
    "The lamb was sure to go.\n",
    "\n",
    "Old MacDonald had a farm,\n",
    "E-I-E-I-O,\n",
    "And on his farm he had a cow,\n",
    "E-I-E-I-O.\n",
    "\n",
    "Itsy Bitsy Spider climbed up the waterspout.\n",
    "Down came the rain and washed the spider out.\n",
    "Out came the sun and dried up all the rain,\n",
    "And the Itsy Bitsy Spider climbed up the spout again.\n",
    "\n",
    "The wheels on the bus go round and round,\n",
    "Round and round,\n",
    "Round and round.\n",
    "The wheels on the bus go round and round,\n",
    "All through the town.\n",
    "\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Preprocess the text data to tokenize and create n-grams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "# Define the value of N for N-grams (context size)\n",
    "N = 2  \n",
    "\n",
    "# Preprocess the text (e.g., nursery rhymes) to tokenize and clean it\n",
    "tokens = preprocess(nursery_rhymes)  # Use the preprocess function to tokenize the text\n",
    "\n",
    "# Generate N-grams using the `genngrams` function\n",
    "ngrams = genngrams(tokens)  \n",
    "\n",
    "# Extract the first context-target pair from the list of N-grams\n",
    "context, target = ngrams[0]  \n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - Convert Context Words into Embeddings and Pass Them Through a Linear Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "# Tokenize text and create vocabulary\n",
    "vocab = tokenizetext(nursery_rhymes)  \n",
    "\n",
    "\n",
    "# Define embedding dimensions and create layers\n",
    "embedding_dim = 20  # Each word will be represented as a 20-dimensional vector\n",
    "linear = nn.Linear(embedding_dim * CONTEXT_SIZE, 128)  \n",
    "\n",
    "\n",
    "# Generate embeddings using the custom embedding function\n",
    "embeddings = genembedding(vocab)  \n",
    "\n",
    "\n",
    "# Convert context words into embeddings\n",
    "my_embeddings = embeddings(torch.tensor(vocab(context)))  \n",
    "\n",
    "\n",
    "# Reshape embeddings to match the input shape required by the linear layer\n",
    "my_embeddings = my_embeddings.reshape(1, -1)  \n",
    "\n",
    "\n",
    "# Pass embeddings through the linear layer\n",
    "output = linear(my_embeddings)  \n",
    "\n",
    "\n",
    "# Print output shape for verification\n",
    "print(\"Output shape:\", output.shape)  \n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 -  Implement Batch Processing with Padding for Language Modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "# Define constants for training\n",
    "CONTEXT_SIZE = 3  # The number of words in the context window\n",
    "BATCH_SIZE = 10   # The number of samples per batch\n",
    "EMBEDDING_DIM = 10  # The dimension of the word embeddings\n",
    "\n",
    "# Compute padding to ensure the number of tokens is evenly divisible by the batch size\n",
    "Padding = BATCH_SIZE - len(tokens) % BATCH_SIZE  \n",
    "\n",
    "\n",
    "tokens_pad = tokens + tokens[0:Padding]  \n",
    "\n",
    "\n",
    "# Define device (CPU/GPU) for computation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
    "\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "dataloader = DataLoader(\n",
    "    tokens_pad, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch\n",
    ")  \n",
    "\n",
    "\n",
    "# Print length of total tokens after padding\n",
    "print(f\"Total tokens (after padding): {len(tokens_pad)}\")\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 - Train an N-gram language model using the provided code structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "# Define context size for N-gram modeling\n",
    "CONTEXT_SIZE = 2  # Number of previous words used as context for predicting the next word\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "\n",
    "# Initialize the N-gram language model\n",
    "model3 = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE).to(device)  \n",
    "\n",
    "\n",
    "# Define the optimizer (Stochastic Gradient Descent)\n",
    "optimizer = optim.SGD(model3.parameters(), lr=0.01)  \n",
    "\n",
    "# Implement Learning Rate Scheduling\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)  \n",
    "\n",
    "# Train the model and track loss history\n",
    "loss_history = train(dataloader, model3, nursery_rhymes)  \n",
    "\n",
    "\n",
    "# Output expected behavior\n",
    "print(f\"Training started with {len(vocab)} words in vocabulary.\")\n",
    "print(f\"Optimizer: {optimizer}\")\n",
    "print(f\"Learning rate scheduler: {scheduler}\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6 - After training, use the model to generate a new nursery rhyme and then print it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "# Select a random line from the nursery rhymes dataset\n",
    "selected_line = pickrandomline(nursery_rhymes)  \n",
    "\n",
    "\n",
    "# Generate a new rhyme using the trained language model\n",
    "generated_rhyme = write_song(model3, selected_line)  \n",
    "\n",
    "\n",
    "# Print the generated rhyme\n",
    "print(generated_rhyme) \n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations! You have completed the lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Joseph Santarcangelo](https://www.linkedin.com/in/joseph-s-50398b136/) has a Ph.D. in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contributor\n",
    "\n",
    "[Roodra Kanwar](https://www.linkedin.com/in/roodrakanwar/) is completing his MS in CS specializing in big data from Simon Fraser University. He has previous experience working with machine learning and as a data engineer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{## Change log}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{|Date (YYYY-MM-DD)|Version|Changed By|Change Description||-|-|-|-||2023-09-01|0.1|Joseph|Created Lab Template & Guided Project||2023-09-03|0.1|Joseph|Updated Guided Project|}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "4671d3551c3b085ee342a48d66878e4c1e78ff04e2d986943d86f04439608e0e"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
